{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Programa para métricas de avaliação por matriz de confusão de um classificador\n",
    "# VP/TP -> VERDADEIRO POSITIVO/TRUE POSITIVE\n",
    "# FP -> FALSO POSITIVO/FALSE POSITIVE\n",
    "# VN/TN -> VERDADEIRO NEGATIVO/TRUE NEGATIVE\n",
    "# FN -> FALSO NEGATIVO/FALSE NEGATIVE\n",
    "# N -> TOTAL DE AMOSTRAS\n",
    "# Accuracy/Acurácia: (VP+VN)/N\n",
    "# Sensitivity/Sensibilidade ou Recall/Revocação: VP/(FN+VP) (True Positive Rate/Taxa de Verdadeiro Positivo)\n",
    "# Specitivity/Especificidade: VN/(FP+VN) (True Negative Rate/Taxa de Verdadeiro Negativo)\n",
    "# Precision/Precisão: VP/(VP+FP)\n",
    "# F-Score: 2*(Precision*Recall/(Precision+Recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsFromCM:\n",
    "    \"\"\"\n",
    "    Cálculo de métricas de avaliação de um modelo de inferência através de uma matriz de confusão. A entrada deve ser um numpy array NxN (e.g. 3x3).\n",
    "    Obtém o número de verdadeiros positivo e negativo e falsos positivo e negativo para cada classe e calcula:\n",
    "    - Acurácia\n",
    "    - Sensibilidade\n",
    "    - Especificidade\n",
    "    - Precisão\n",
    "    - F1-score\n",
    "    Por fim, calcula as métricas globais do modelo:\n",
    "    - Micro F1-score\n",
    "    - Macro F1-score\n",
    "    - Macro F1-score com pesos para cada classe inferida\n",
    "    \"\"\"\n",
    "    def __init__(self, cm):\n",
    "        if hasattr(cm, \"shape\"):\n",
    "            assert len(cm.shape) == 2 and (cm.shape[0] == cm.shape[1]), 'Invalid Confusion Matrix - (N x N) numpy array is expected'\n",
    "            self.ConfusionMatrix = cm \n",
    "            self.ClassNumber = cm.shape[0]\n",
    "            self.classInferred = []\n",
    "            self.classMetrics = []\n",
    "            for i in range(self.ClassNumber):\n",
    "                Samples = np.sum(self.ConfusionMatrix[:, i])\n",
    "                TP = self.ConfusionMatrix[i][i]\n",
    "                FP = np.sum(np.delete(self.ConfusionMatrix[:, i], i))\n",
    "                TN = np.sum(np.delete(np.delete(self.ConfusionMatrix, i, 0), i, 1).flatten())\n",
    "                FN = np.sum(np.delete(self.ConfusionMatrix[i, :], i))\n",
    "                Accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "                Specitivity = TN/(FP+TN)\n",
    "                Precision = TP/(TP+FP)\n",
    "                Recall = TP/(FN+TP)\n",
    "                Fscore = 2*(Precision*Recall)/(Precision+Recall)\n",
    "                self.classInferred.append({'Real Samples': Samples, 'TP': TP, 'FP': FP, 'TN': TN, 'FN': FN})\n",
    "                self.classMetrics.append({'Accuracy': Accuracy, 'Specitivity': Specitivity, 'Precision': Precision, 'Recall': Recall, 'F1-Score': Fscore})\n",
    "            \n",
    "            self.Globals = {'Micro F1-Score': self.MicroF1(), 'Macro F1-Score': self.MacroF1(), 'Macro Weighted F1-Score': self.WeightedF1()}\n",
    "            print('Confusion Matrix:')\n",
    "            print(self.ConfusionMatrix)\n",
    "            print('Inferences: {} samples'.format(np.sum(self.ConfusionMatrix.flatten())))\n",
    "            print('Classification: {} classes'.format(self.ClassNumber))\n",
    "            print('===========================================================')\n",
    "            print('Global Scores:')\n",
    "            globals = pd.DataFrame(self.Globals.values(), self.Globals.keys()).T.to_string(index=False)\n",
    "            print(globals)\n",
    "            print('===========================================================')\n",
    "            print('Class Metrics:')\n",
    "            print(pd.DataFrame(self.classMetrics))\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print('Invalid Input')  \n",
    "\n",
    "    def MicroF1(self) -> float:\n",
    "        TP = []\n",
    "        FP = []\n",
    "        FN = []\n",
    "        for item in self.classInferred:\n",
    "            for key in item.keys():\n",
    "                if key == 'TP':\n",
    "                    TP.append(item[key])                    \n",
    "                if key == 'FP':\n",
    "                    FP.append(item[key])                        \n",
    "                if key == 'FN':\n",
    "                    FN.append(item[key])\n",
    "        TP = sum(TP)\n",
    "        FP = sum(FP)\n",
    "        FN = sum(FN)\n",
    "        Precision = TP/(TP+FP)\n",
    "        Recall = TP/(FN+TP)\n",
    "\n",
    "        return 2*(Precision*Recall)/(Precision+Recall)\n",
    "    \n",
    "    def MacroF1(self) -> float:\n",
    "        Fscores = [item['F1-Score'] for item in self.classMetrics]\n",
    "        return sum(Fscores)/len(Fscores)\n",
    "    \n",
    "    def WeightedF1(self) -> float:\n",
    "        Samples = [item['Real Samples'] for item in self.classInferred]\n",
    "        Fscores = [item['F1-Score'] for item in self.classMetrics]\n",
    "        Weighted = [Samples[i]*Fscores[i] for i in range(self.ClassNumber)]\n",
    "\n",
    "        return sum(Weighted)/sum(Samples)\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 3  1  0  4]\n",
      " [ 0  4  3  1]\n",
      " [ 0  2  2  6]\n",
      " [ 3  1  1 10]]\n",
      "Inferences: 41 samples\n",
      "Classification: 4 classes\n",
      "===========================================================\n",
      "Global Scores:\n",
      " Micro F1-Score  Macro F1-Score  Macro Weighted F1-Score\n",
      "       0.463415        0.433532                 0.481417\n",
      "===========================================================\n",
      "Class Metrics:\n",
      "   Accuracy  Specitivity  Precision    Recall  F1-Score\n",
      "0  0.804878     0.909091   0.500000  0.375000  0.428571\n",
      "1  0.804878     0.878788   0.500000  0.500000  0.500000\n",
      "2  0.707317     0.870968   0.333333  0.200000  0.250000\n",
      "3  0.609756     0.576923   0.476190  0.666667  0.555556\n"
     ]
    }
   ],
   "source": [
    "dummy_cm = np.array([[3, 1, 0, 4], [0, 4, 3, 1], [0, 2, 2, 6], [3, 1, 1, 10]])\n",
    "#dummy_cm = np.array([[10, 1], [3, 7]])\n",
    "#dummy_cm = np.identity(3)\n",
    "metrics = MetricsFromCM(dummy_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78ddfc3686b8b7161f2836984651df038ec9a0366954334fc42499f59ad2b3c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
